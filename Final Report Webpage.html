<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.549">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Final Report</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
</style>


<script src="Final Report Webpage_files/libs/clipboard/clipboard.min.js"></script>
<script src="Final Report Webpage_files/libs/quarto-html/quarto.js"></script>
<script src="Final Report Webpage_files/libs/quarto-html/popper.min.js"></script>
<script src="Final Report Webpage_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="Final Report Webpage_files/libs/quarto-html/anchor.min.js"></script>
<link href="Final Report Webpage_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="Final Report Webpage_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="Final Report Webpage_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="Final Report Webpage_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="Final Report Webpage_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Final Report</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<div class="cell">
<div class="cell-output cell-output-stderr">
<pre><code>
Attaching package: 'dplyr'</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:stats':

    filter, lag</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>The following objects are masked from 'package:base':

    intersect, setdiff, setequal, union</code></pre>
</div>
<div class="cell-output cell-output-stderr">
<pre><code>── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ forcats   1.0.0     ✔ readr     2.1.4
✔ ggplot2   3.5.0     ✔ stringr   1.5.0
✔ lubridate 1.9.3     ✔ tibble    3.2.1
✔ purrr     1.0.2     ✔ tidyr     1.3.0
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors

Attaching package: 'plotly'


The following object is masked from 'package:ggplot2':

    last_plot


The following object is masked from 'package:stats':

    filter


The following object is masked from 'package:graphics':

    layout



Attaching package: 'psych'


The following objects are masked from 'package:ggplot2':

    %+%, alpha



Attaching package: 'reshape2'


The following object is masked from 'package:tidyr':

    smiths


randomForest 4.7-1.1

Type rfNews() to see new features/changes/bug fixes.


Attaching package: 'randomForest'


The following object is masked from 'package:psych':

    outlier


The following object is masked from 'package:ggplot2':

    margin


The following object is masked from 'package:dplyr':

    combine


Loading required package: lattice


Attaching package: 'caret'


The following object is masked from 'package:purrr':

    lift</code></pre>
</div>
</div>
<section id="abstract" class="level2">
<h2 class="anchored" data-anchor-id="abstract">Abstract</h2>
<p>For the final project for our STAT 515 class, we were asked to find a dataset and perform a robust advanced analysis on the data. For this project we chose to use breast cancer screening data from the University of Wisconsin [1].&nbsp;This study aims to identify patterns and indicators that could potentially predict the signs of malignant cancerous cells. Through meticulous analysis, the paper investigates various aspects, including a deep dive into the predictor variables and their relationship to each other and the response variable, the creation of various statistical models to predict cancerous cells and then finally a comparison of models to choose the best one. By scrutinizing these factors, this research hopes to make discovering breast cancer in patients easier and earlier. &nbsp;</p>
</section>
<section id="i.-introduction" class="level2">
<h2 class="anchored" data-anchor-id="i.-introduction">i. Introduction</h2>
<p>Breast cancer is one of the most common cancers in the world. While the pharmaceutical industry has invested quite a bit in trying to find a definitive cure to this cancer, it still raises the need for more analysis to be done on breast cancer data so that a cancerous tumor can be caught in the early stages. We wanted to see if it was possible to identify the potential emergence of breast cancer in women based on different features of the tumor cells. This dataset seemed like a good fit since it has 9 predictor variables, and the outcome variable would indicate whether the tumor can be classified as malignant (cancerous) or benign (non-cancerous).&nbsp;</p>
<p>The dataset we choose to perform this research is from Dr.&nbsp;William Wolberg and his clinical studies from 1989 to 1991. This dataset is very well known and highly integrable due to the amount of research conducted using this data. Below is a list and description of the 9 predictor variables and the 1 response variable (benormal).&nbsp;&nbsp;</p>
<ul>
<li><p><strong>clumpthickness</strong>: (1-10). Benign cells tend to be grouped in monolayers, while cancerous cells are often grouped in multilayers.&nbsp;</p></li>
<li><p><strong>uniformcellsize</strong> (1-10). Cancer cells tend to vary in size and shape.&nbsp;</p></li>
<li><p><strong>uniformcellshape</strong> (1-10). Cancer cells tend to vary in shape and size.&nbsp;</p></li>
<li><p><strong>margadhesion</strong>: (1-10). Normal cells tend to stick together, while cancer cells tend to lose this ability, so the loss of adhesion is a sign of malignancy.&nbsp;</p></li>
<li><p><strong>epithelial</strong>: (1-10). It is related to the uniformity mentioned above. Epithelial cells that are significantly enlarged may be malignant.&nbsp;</p></li>
<li><p><strong>barenuclei</strong>: (1-10). This term is used for nuclei not surrounded by cytoplasm (the rest of the cell). Those are typically seen in benign tumors.&nbsp;</p></li>
<li><p><strong>blandchromatin</strong>: (1-10). Describes a uniform “texture” of the nucleus seen in benign cells. In cancer cells, the chromatin tends to be more coarse and to form clumps.&nbsp;</p></li>
<li><p><strong>normalnucleoli</strong>: (1-10). Nucleoli are small structures seen in the nucleus. In normal cells, the nucleolus is usually very small, if visible. The nucleoli become more prominent in cancer cells, and sometimes there are multiple.&nbsp;</p></li>
<li><p><strong>mitoses</strong>: (1-10). Cancer is essentially a disease of uncontrolled mitosis.&nbsp;</p></li>
<li><p><strong>benormal</strong>: (2 or 4). Benign (non-cancerous) or malignant (cancerous) lump in a breast.&nbsp;ii. Materials and Methods</p></li>
</ul>
<p>The University of Wisconsin breast cancer data from William Wolberg has 699 observations and 10 variables, the first variable represents the ID of the sample and the last column “benornal” represents the classification/response variable (for benign, 4 for malignant).</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>'data.frame':   699 obs. of  11 variables:
 $ id              : int  1000025 1002945 1015425 1016277 1017023 1017122 1018099 1018561 1033078 1033078 ...
 $ clumpthickness  : int  5 5 3 6 4 8 1 2 2 4 ...
 $ uniformcellsize : int  1 4 1 8 1 10 1 1 1 2 ...
 $ uniformcellshape: int  1 4 1 8 1 10 1 2 1 1 ...
 $ margadhesion    : int  1 5 1 1 3 8 1 1 1 1 ...
 $ epithelial      : int  2 7 2 3 2 7 2 2 2 2 ...
 $ barenuclei      : chr  "1" "10" "2" "4" ...
 $ blandchromatin  : int  3 3 3 3 3 9 3 3 1 2 ...
 $ normalnucleoli  : int  1 2 1 7 1 7 1 1 1 1 ...
 $ mitoses         : int  1 1 1 1 1 1 1 1 5 1 ...
 $ benormal        : int  2 2 2 2 2 4 2 2 2 2 ...</code></pre>
</div>
</div>
<p>To make the dataset ready for analysis we removed the ID column, checked and removed all rows with missing data changed he response variable values to malignant (4) = 1 and benign (2) = 0.</p>
<div class="cell">
<div class="cell-output cell-output-stderr">
<pre><code>Warning: NAs introduced by coercion</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>  clumpthickness  uniformcellsize uniformcellshape     margadhesion 
               0                0                0                0 
      epithelial       barenuclei   blandchromatin   normalnucleoli 
               0               16                0                0 
         mitoses         benormal 
               0                0 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>'data.frame':   683 obs. of  10 variables:
 $ clumpthickness  : int  5 5 3 6 4 8 1 2 2 4 ...
 $ uniformcellsize : int  1 4 1 8 1 10 1 1 1 2 ...
 $ uniformcellshape: int  1 4 1 8 1 10 1 2 1 1 ...
 $ margadhesion    : int  1 5 1 1 3 8 1 1 1 1 ...
 $ epithelial      : int  2 7 2 3 2 7 2 2 2 2 ...
 $ barenuclei      : int  1 10 2 4 1 10 10 1 1 1 ...
 $ blandchromatin  : int  3 3 3 3 3 9 3 3 1 2 ...
 $ normalnucleoli  : int  1 2 1 7 1 7 1 1 1 1 ...
 $ mitoses         : int  1 1 1 1 1 1 1 1 5 1 ...
 $ benormal        : int  2 2 2 2 2 4 2 2 2 2 ...
 - attr(*, "na.action")= 'omit' Named int [1:16] 24 41 140 146 159 165 236 250 276 293 ...
  ..- attr(*, "names")= chr [1:16] "24" "41" "140" "146" ...</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> clumpthickness   uniformcellsize  uniformcellshape  margadhesion  
 Min.   : 1.000   Min.   : 1.000   Min.   : 1.000   Min.   : 1.00  
 1st Qu.: 2.000   1st Qu.: 1.000   1st Qu.: 1.000   1st Qu.: 1.00  
 Median : 4.000   Median : 1.000   Median : 1.000   Median : 1.00  
 Mean   : 4.442   Mean   : 3.151   Mean   : 3.215   Mean   : 2.83  
 3rd Qu.: 6.000   3rd Qu.: 5.000   3rd Qu.: 5.000   3rd Qu.: 4.00  
 Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :10.00  
   epithelial       barenuclei     blandchromatin   normalnucleoli 
 Min.   : 1.000   Min.   : 1.000   Min.   : 1.000   Min.   : 1.00  
 1st Qu.: 2.000   1st Qu.: 1.000   1st Qu.: 2.000   1st Qu.: 1.00  
 Median : 2.000   Median : 1.000   Median : 3.000   Median : 1.00  
 Mean   : 3.234   Mean   : 3.545   Mean   : 3.445   Mean   : 2.87  
 3rd Qu.: 4.000   3rd Qu.: 6.000   3rd Qu.: 5.000   3rd Qu.: 4.00  
 Max.   :10.000   Max.   :10.000   Max.   :10.000   Max.   :10.00  
    mitoses          benormal     
 Min.   : 1.000   Min.   :0.0000  
 1st Qu.: 1.000   1st Qu.:0.0000  
 Median : 1.000   Median :0.0000  
 Mean   : 1.603   Mean   :0.3499  
 3rd Qu.: 1.000   3rd Qu.:1.0000  
 Max.   :10.000   Max.   :1.0000  </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
  0   1 
444 239 </code></pre>
</div>
</div>
<p>After cleaning the dataset, we looked at a summary of the statistics for each variable. For our sample there are 444 records that are identified as not being malignant (=0) and 239 records that are identified as being malignant (=1).</p>
<p>We next wanted to identify if there were any patterns amongst the predictor variables in the dataset. First we looked at the correlations, histrograms and scatterplots of the variables using the pairs.panel() function.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Final-Report-Webpage_files/figure-html/unnamed-chunk-4-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Some of the variables showed correlations to each other, but none were deemed significant by the corr.test() function. But one thing we did notice was that many of the variables exhibited a right skew with their means larger than their medians. We could potentially log() the variables to make them more uniform.</p>
</section>
<section id="ii.-materials-and-methods" class="level2">
<h2 class="anchored" data-anchor-id="ii.-materials-and-methods">ii. Materials and Methods</h2>
<section id="logistic-regression-model" class="level3">
<h3 class="anchored" data-anchor-id="logistic-regression-model">Logistic Regression Model</h3>
<p>We decided to develop a Logisitic regression model to see if we could develop a model that could be used to predict if a cell was cancerous or not. Because our outcome can only be one of two things (cell is malignant or benign) we should be using a classification model and logistic regression is a simple model which is much easier to set up and train initially than other machine learning models.</p>
<p>For the first model we used the cleaned dataset and all of the variables.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = benormal ~ ., family = binomial, data = df_clean)

Coefficients:
                  Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      -10.10394    1.17488  -8.600  &lt; 2e-16 ***
clumpthickness     0.53501    0.14202   3.767 0.000165 ***
uniformcellsize   -0.00628    0.20908  -0.030 0.976039    
uniformcellshape   0.32271    0.23060   1.399 0.161688    
margadhesion       0.33064    0.12345   2.678 0.007400 ** 
epithelial         0.09663    0.15659   0.617 0.537159    
barenuclei         0.38303    0.09384   4.082 4.47e-05 ***
blandchromatin     0.44719    0.17138   2.609 0.009073 ** 
normalnucleoli     0.21303    0.11287   1.887 0.059115 .  
mitoses            0.53484    0.32877   1.627 0.103788    
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 884.35  on 682  degrees of freedom
Residual deviance: 102.89  on 673  degrees of freedom
AIC: 122.89

Number of Fisher Scoring iterations: 8</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:  glm(formula = benormal ~ ., family = binomial, data = df_clean)

Coefficients:
     (Intercept)    clumpthickness   uniformcellsize  uniformcellshape  
       -10.10394           0.53501          -0.00628           0.32271  
    margadhesion        epithelial        barenuclei    blandchromatin  
         0.33064           0.09664           0.38302           0.44719  
  normalnucleoli           mitoses  
         0.21303           0.53484  

Degrees of Freedom: 682 Total (i.e. Null);  673 Residual
Null Deviance:      884.4 
Residual Deviance: 102.9    AIC: 122.9</code></pre>
</div>
</div>
<p>In this model the variables clumpthickness, margadhesion, barenuclei, blandchromatin were considered the only variables had a significant impact on the response variables with p-values less than .05. The overall model had an AIC of 122.89.</p>
<p>Next, we wanted to create a new model after logging our variables.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = benormal ~ ., family = binomial, data = df_log)

Coefficients:
                 Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)      -9.31304    1.25953  -7.394 1.42e-13 ***
clumpthickness    1.80864    0.59805   3.024  0.00249 ** 
uniformcellsize   0.50056    0.66380   0.754  0.45080    
uniformcellshape  1.26038    0.74379   1.695  0.09016 .  
margadhesion      0.71750    0.39504   1.816  0.06933 .  
epithelial       -0.04541    0.63630  -0.071  0.94310    
barenuclei        0.36117    0.09149   3.948 7.89e-05 ***
blandchromatin    1.49565    0.61321   2.439  0.01473 *  
normalnucleoli    0.48867    0.35560   1.374  0.16938    
mitoses           1.34541    0.74040   1.817  0.06920 .  
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 884.35  on 682  degrees of freedom
Residual deviance: 107.02  on 673  degrees of freedom
AIC: 127.02

Number of Fisher Scoring iterations: 8</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:  glm(formula = benormal ~ ., family = binomial, data = df_log)

Coefficients:
     (Intercept)    clumpthickness   uniformcellsize  uniformcellshape  
        -9.31304           1.80864           0.50056           1.26038  
    margadhesion        epithelial        barenuclei    blandchromatin  
         0.71750          -0.04541           0.36117           1.49565  
  normalnucleoli           mitoses  
         0.48867           1.34541  

Degrees of Freedom: 682 Total (i.e. Null);  673 Residual
Null Deviance:      884.4 
Residual Deviance: 107  AIC: 127</code></pre>
</div>
</div>
<p>For this model the variables clumpthickness and barenuclei were considered the only variables had a significant impact on the response variables with p-values less than .05. The overall model had an AIC of 127.02.</p>
<p>Based on the AIC of these two models, the non-logged model performed better. Now lets try and simplify the model.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
glm(formula = benormal ~ clumpthickness + margadhesion + barenuclei + 
    blandchromatin, family = binomial, data = df_clean)

Coefficients:
                Estimate Std. Error z value Pr(&gt;|z|)    
(Intercept)    -10.11370    1.03264  -9.794  &lt; 2e-16 ***
clumpthickness   0.81166    0.12585   6.450 1.12e-10 ***
margadhesion     0.43412    0.11403   3.807 0.000141 ***
barenuclei       0.48136    0.08816   5.460 4.76e-08 ***
blandchromatin   0.70154    0.15196   4.616 3.90e-06 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1

(Dispersion parameter for binomial family taken to be 1)

    Null deviance: 884.35  on 682  degrees of freedom
Residual deviance: 125.77  on 678  degrees of freedom
AIC: 135.77

Number of Fisher Scoring iterations: 8</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:  glm(formula = benormal ~ clumpthickness + margadhesion + barenuclei + 
    blandchromatin, family = binomial, data = df_clean)

Coefficients:
   (Intercept)  clumpthickness    margadhesion      barenuclei  blandchromatin  
      -10.1137          0.8117          0.4341          0.4814          0.7015  

Degrees of Freedom: 682 Total (i.e. Null);  678 Residual
Null Deviance:      884.4 
Residual Deviance: 125.8    AIC: 135.8</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Analysis of Deviance Table

Model 1: benormal ~ clumpthickness + uniformcellsize + uniformcellshape + 
    margadhesion + epithelial + barenuclei + blandchromatin + 
    normalnucleoli + mitoses
Model 2: benormal ~ clumpthickness + margadhesion + barenuclei + blandchromatin
  Resid. Df Resid. Dev Df Deviance  Pr(&gt;Chi)    
1       673     102.89                          
2       678     125.78 -5  -22.886 0.0003549 ***
---
Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1</code></pre>
</div>
</div>
<p>As should be expected, creating a model using only the significant variables was worse at explaining the dataset. Using the Chisq test, we can also conclude that the more complex model is significantly better than the simpler model.</p>
<p>Using model_1 as the final model, lets see how accurate it is for prediction.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>          1           2           3           4           5           6 
0.016046581 0.908808622 0.008137623 0.760934919 0.018166848 0.999973622 
          7           8           9          10 
0.056844170 0.004503358 0.011249056 0.006032371 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction   0   1
         0 434  11
         1  10 228
                                          
               Accuracy : 0.9693          
                 95% CI : (0.9534, 0.9809)
    No Information Rate : 0.6501          
    P-Value [Acc &gt; NIR] : &lt;2e-16          
                                          
                  Kappa : 0.9324          
                                          
 Mcnemar's Test P-Value : 1               
                                          
            Sensitivity : 0.9775          
            Specificity : 0.9540          
         Pos Pred Value : 0.9753          
         Neg Pred Value : 0.9580          
             Prevalence : 0.6501          
         Detection Rate : 0.6354          
   Detection Prevalence : 0.6515          
      Balanced Accuracy : 0.9657          
                                          
       'Positive' Class : 0               
                                          </code></pre>
</div>
</div>
<p>Looking at the confusion matrix this is a very good model with high predictability for both false positives and negatives. The % chance the model falsely predicted cancer when not was 10/444 (2.2%) and the chance the model falsely predicted not having cancer when there was 11/239 (4.6%).</p>
</section>
<section id="decision-tree-and-random-forest-modeling" class="level3">
<h3 class="anchored" data-anchor-id="decision-tree-and-random-forest-modeling">Decision Tree and Random Forest Modeling</h3>
<p>Another modeling type we used for trying to predict whether a cell was cancerous or not was a random forest model. One of the biggest advantages of random forests is its versatility. It can be used for both regression and classification tasks, and it’s also easy to view the relative importance it assigns to the input features. One of the biggest problems in machine learning is overfitting, but most of the time this won’t happen thanks to the random forest classifier. If there are enough trees in the forest, the classifier won’t overfit the model.</p>
<p>First, we looked at a decision tree model to determine the best spilt for node splitting. We did this by splitting the data into two sets, training and testing to train the model and then test its accuracy.</p>
<div class="cell">
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Final-Report-Webpage_files/figure-html/unnamed-chunk-9-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>Based on these results, it is best to include 6 variables in each split. Knowing this, we created a random forest model at the desired variable split.</p>
<div class="cell">
<div class="cell-output cell-output-stderr">
<pre><code>Warning in randomForest.default(m, y, ...): The response has five or fewer
unique values.  Are you sure you want to do regression?</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] 0.03088145</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>
Call:
 randomForest(formula = benormal ~ ., data = df_clean, mtry = 6,      importance = TRUE, subset = train) 
               Type of random forest: regression
                     Number of trees: 500
No. of variables tried at each split: 6

          Mean of squared residuals: 0.0294727
                    % Var explained: 86.76</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Final-Report-Webpage_files/figure-html/unnamed-chunk-10-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Final-Report-Webpage_files/figure-html/unnamed-chunk-10-2.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The variable that is most important to reduce the mean standard error (MSE) is barenuceli and the variables that were deemed the most important to include in the node splitting were uniformcellsize and uniformcellshape. Overall, the model is very good with 88.7% of the variables explained. Looking at the confusion matrix:</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>            1             2             3             4             5 
-6.311618e-16  7.780000e-01 -5.944134e-16  3.439000e-01  4.533333e-03 
            6             7             8             9            10 
 1.000000e+00  2.551000e-01 -6.267209e-16  2.292000e-01 -5.822010e-16 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>Confusion Matrix and Statistics

          Reference
Prediction   0   1
         0 438   9
         1   6 230
                                         
               Accuracy : 0.978          
                 95% CI : (0.964, 0.9877)
    No Information Rate : 0.6501         
    P-Value [Acc &gt; NIR] : &lt;2e-16         
                                         
                  Kappa : 0.9516         
                                         
 Mcnemar's Test P-Value : 0.6056         
                                         
            Sensitivity : 0.9865         
            Specificity : 0.9623         
         Pos Pred Value : 0.9799         
         Neg Pred Value : 0.9746         
             Prevalence : 0.6501         
         Detection Rate : 0.6413         
   Detection Prevalence : 0.6545         
      Balanced Accuracy : 0.9744         
                                         
       'Positive' Class : 0              
                                         </code></pre>
</div>
</div>
<p>The percent chance the model falsely predicted cancer when not was 6/444 (1.4%) and the chance the model falsely predicted not having cancer when there was 9/239 (3.7%).</p>
</section>
<section id="principal-component-analysis" class="level3">
<h3 class="anchored" data-anchor-id="principal-component-analysis">Principal Component Analysis</h3>
<p>We mentioned earlier that some of our variables were correlated but none were significantly correlated to each other. Also, in some of our other models, some of the variables were not significantly correlated to predicting the response variable. To examine this further, we did a principal component analysis on the predictor variables too see if reducing the number of variables of a data set naturally comes at the expense of accuracy while not losing too much accuracy.</p>
<div class="cell">
<div class="cell-output cell-output-stdout">
<pre><code>  clumpthickness  uniformcellsize uniformcellshape     margadhesion 
        4.442167         3.150805         3.215227         2.830161 
      epithelial       barenuclei   blandchromatin   normalnucleoli 
        3.234261         3.544656         3.445095         2.869693 
         mitoses         benormal 
        1.603221         1.349927 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>  clumpthickness  uniformcellsize uniformcellshape     margadhesion 
       7.9566944        9.3951130        8.9316153        8.2057165 
      epithelial       barenuclei   blandchromatin   normalnucleoli 
       4.9421089       13.2776950        6.0010133        9.3187722 
         mitoses         benormal 
       3.0021597        0.2278116 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>[1] "sdev"     "rotation" "center"   "scale"    "x"       </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>  clumpthickness  uniformcellsize uniformcellshape     margadhesion 
        4.442167         3.150805         3.215227         2.830161 
      epithelial       barenuclei   blandchromatin   normalnucleoli 
        3.234261         3.544656         3.445095         2.869693 
         mitoses         benormal 
        1.603221         1.349927 </code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>         PC1         PC2         PC3         PC4         PC5         PC6
1   1.632138 -0.10110899  0.53066017  0.04867467 -0.13967887  0.27266403
2  -1.091952 -0.36764796 -0.37608283 -0.33727673  1.66159818 -0.49947533
3   1.747215 -0.06835623 -0.05452903 -0.08442398 -0.05762203 -0.12479945
4  -1.123580 -0.30561252  0.32020004  1.60389615 -0.54780477  0.16977170
5   1.517047 -0.06200488 -0.04883144 -0.28957594 -0.09371811  0.57460157
6  -5.173918 -1.36302344 -0.65367495  0.40382274  0.40838925  0.39318524
7   1.250797 -0.53179083 -0.59909575 -1.16253763  0.26500705 -1.70934020
8   1.817759  0.03688650 -0.35718994  0.11869407 -0.03111154 -0.03121061
9   1.714243  2.28289475  0.16547803 -0.59033209 -0.12926420 -0.30699255
10  1.749457  0.02462355  0.31567378  0.12332678  0.02349157  0.20775672
           PC7         PC8         PC9        PC10
1   0.32271338 -0.41613912  0.14987849 -0.01320316
2  -0.76488363 -0.51190377  1.43717566 -0.18341430
3   0.33212748 -0.22014329  0.14173141 -0.02990047
4  -0.18141468  1.53841728  1.21084570 -0.32073360
5   0.09877523 -0.42777805  0.09395639  0.02496084
6   0.34574434  0.13590736  0.52396579 -0.01704468
7   0.10416365  0.10310453  0.89835584 -0.27270693
8   0.41671423  0.04563756  0.03920288  0.22753908
9   0.16385020  0.27850361 -0.07680255 -0.01644927
10  0.07179972 -0.02809058  0.05914645 -0.26772940</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Final-Report-Webpage_files/figure-html/unnamed-chunk-12-1.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 6.73117831 0.79315381 0.54596815 0.46529869 0.38038147 0.31254327
 [7] 0.29602826 0.26121943 0.12634121 0.08788742</code></pre>
</div>
<div class="cell-output cell-output-stdout">
<pre><code> [1] 0.673117831 0.079315381 0.054596815 0.046529869 0.038038147 0.031254327
 [7] 0.029602826 0.026121943 0.012634121 0.008788742</code></pre>
</div>
<div class="cell-output-display">
<div>
<figure class="figure">
<p><img src="Final-Report-Webpage_files/figure-html/unnamed-chunk-12-2.png" class="img-fluid figure-img" width="672"></p>
</figure>
</div>
</div>
</div>
<p>The eigenvalues taper off after the first few components, with the first two capturing the majority of the variance.</p>
<p>In the plot on the right we can see that the first 6 components explain over 90% of the variance in the data and the remaining components add little additional information.</p>
</section>
</section>
<section id="iii.-limitations" class="level2">
<h2 class="anchored" data-anchor-id="iii.-limitations">iii. Limitations</h2>
<p>There were no major limitations for this analysis but there were a few of things that needed to be done to the dataset in order to clean and make the dataset usable for analysis. First, we had to identify and remove all missing rows from the dataset. Second, we had to transform the response variable to be "0" and "1". Finally, many of the variables seemed to show a left skew making us question if the variables should be transformed or not.&nbsp;</p>
<p>There were only 699 samples for the dataset and although a solid number of samples, more samples would lead to a more predictable conclusion.&nbsp;</p>
</section>
<section id="iv.-conclusion" class="level2">
<h2 class="anchored" data-anchor-id="iv.-conclusion">iv. Conclusion</h2>
<p>It seems that a significantly accurate model to predict if a cancerous cell could be malignant using the measurements recorded in this dataset. To test this, we created multiple regression models, a random forest model, and a PCA analysis to understand the variables more thoroughly. &nbsp;</p>
<p>In the PCA test we determined that the dataset's variance could be explained by simplifying and using only 2 of 9 predictor variables. We then created multiple logistic regression models and compared them to each other to choose the best one. After variable transformation and selection, we determined the best model would be to use all the variables. The percent chance the model falsely predicted cancer when not was 10/444 (2.2%) and the chance the model falsely predicted not having cancer when there was 11/239 (4.6%). We then decided to try and create an optimal best random forest model, starting with a best fit decision tree model. The created model resulted in an overall accuracy rate of almost 98% with the percent chance the model falsely predicted cancer when not was 6/444 (1.4%) and the chance the model falsely predicted not having cancer when there was 9/239 (3.7%). Based on the two models we created, the best model to use would be the random forest model because of its high accuracy and predictability.&nbsp;</p>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<p>[1] Wolberg,WIlliam. (1992). Breast Cancer Wisconsin (Original). UCI Machine Learning Repository. https://doi.org/10.24432/C5HP4Z.&nbsp;</p>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>